{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, KFold\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "sns.set()\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import pprint\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets as dat\n",
    "from sklearn import pipeline as pip\n",
    "from sklearn import linear_model as lin\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble as ens\n",
    "from sklearn import preprocessing as pre\n",
    "from sklearn import model_selection as mod\n",
    "from sklearn import metrics as met\n",
    "from imblearn import over_sampling as ove\n",
    "from imblearn import under_sampling as und\n",
    "from imblearn import combine as com\n",
    "from imblearn import pipeline as imbPipe\n",
    "# Classifiers\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Skopt functions\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/Asus/Downloads/trainproject.csv')\n",
    "#col_names = df.columns.tolist()\n",
    "#col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This class is prepared with the purpose of completing eda and feature exraction steps under the same umbrella. There is no data manipulation is made by causing of data leakage. Detailed info about the steps is written in class itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class imputer_and_generator():\n",
    "    \n",
    "    def _init_():\n",
    "        \"initial\"\n",
    "    def _main_():\n",
    "        print(\"Feature Selection Process is started\")\n",
    "\n",
    "        def column_imputer():\n",
    "            global result\n",
    "            col_imputation = ['WorkChallengeFrequencyPolitics','WorkChallengeFrequencyUnusedResults',\n",
    "                              'WorkChallengeFrequencyDirtyData','WorkChallengeFrequencyExplaining','WorkChallengeFrequencyTalent',\n",
    "                              'WorkChallengeFrequencyClarity','WorkChallengeFrequencyDataAccess','WorkMethodsFrequencyCross-Validation',\n",
    "                              'WorkMethodsFrequencyDataVisualization','WorkMethodsFrequencyDecisionTrees', 'WorkMethodsFrequencyLogisticRegression', \n",
    "                              'WorkMethodsFrequencyNeuralNetworks', 'WorkMethodsFrequencyPCA','WorkMethodsFrequencyRandomForests', \n",
    "                              'WorkMethodsFrequencyTimeSeriesAnalysis',\"WorkToolsFrequencyPython\", \"WorkToolsFrequencyR\", \"WorkToolsFrequencySQL\"]\n",
    "            #dfnew =  df[col_imputation].add_prefix('ORJ_')\n",
    "            #result = pd.concat([df, dfnew], axis=1)\n",
    "            result = df\n",
    "            result[col_imputation] = result[col_imputation].fillna('Missing') #buraya col_imputation ekledim--ali\n",
    "            for i in result[col_imputation]:\n",
    "                result[i] = result[i].map({\n",
    "                                          'Most of the time' : 4,\n",
    "                                          'Often' : 3,\n",
    "                                          'Sometimes' : 2,\n",
    "                                          'Rarely' : 1,\n",
    "                                          'Missing': 0\n",
    "                                          })\n",
    "            \n",
    "            return  result\n",
    "\n",
    "  \n",
    "        def worktoolsfreqmean():\n",
    "            global result\n",
    "            result['mean_of_WorkTools']= result.filter(regex='WorkToolsFrequency').mean(axis=1)\n",
    "            return result\n",
    "        \n",
    "        \n",
    "\n",
    "        def workchallenge():\n",
    "            global result\n",
    "            result['mean_of_the_workchallenge']= result.filter(regex='WorkChallengeFrequency').mean(axis=1) #workchallenge ile değiştirilecek\n",
    "            result['mean_of_the_workmethods']= result.filter(regex='WorkMethodsFrequency').mean(axis=1)\n",
    "            return result        \n",
    "        \n",
    "\n",
    "        def visualization(result):\n",
    "          result['WorkDataVisualizations'] =  result['WorkDataVisualizations'].fillna('None') #burada 29 tane missing vardı. onları da olarak değiştirdim--ali\n",
    "          result['WorkDataVisualizationsbinned'] = result['WorkDataVisualizations'].map({\n",
    "                'None' : 0,\n",
    "                'Less than 10% of projects' : 0.05,\n",
    "                '10-25% of projects' : ((10+25)/2) / 100, #scale ederek ölçeği 0-1 arasına getirdim--ali\n",
    "                '26-50% of projects' : ((26+50)/2) / 100,\n",
    "                '51-75% of projects' : ((51+75)/2) / 100,\n",
    "                '76-99% of projects' : ((76+99)/2) / 100,\n",
    "                '100% of projects' : 1\n",
    "                        })\n",
    "          result.drop('WorkDataVisualizations', axis=1, inplace=True) #orijinal değişkeni nihai tablodan çıkardım--ali\n",
    "          return result\n",
    "        \n",
    "        def externalvsinternaltools(result):\n",
    "          result[\"WorkInternalVsExternalTools\"] = result[\"WorkInternalVsExternalTools\"].fillna(\"Do not know\") # 116 tane missing vardı. Do not know ile değiştirdim--ali\n",
    "          result['UsageofExternaltools'] = result['WorkInternalVsExternalTools'].map({ \n",
    "            'Entirely external' : 1,\n",
    "            'More external than internal' : 0.75,\n",
    "            'Approximately half internal and half external' : 0.50,\n",
    "            'More internal than external' : 0.25,\n",
    "            'Do not know' : 0.10,\n",
    "            'Entirely internal' : 0        \n",
    "                                    })\n",
    "\n",
    "          result['UsageofInternaltools'] = result['WorkInternalVsExternalTools'].map({\n",
    "                'Entirely internal' : 1,\n",
    "                'More internal than external' : 0.75,\n",
    "                'Approximately half internal and half external' : 0.50,\n",
    "                'More external than internal' : 0.25,\n",
    "                'Do not know' : 0.10,\n",
    "                'Entirely external' : 0\n",
    "                                         })\n",
    "          result.drop('WorkInternalVsExternalTools', axis=1, inplace=True) #nihai tablodan çıkardım--ali\n",
    "          return result\n",
    "        \n",
    "        def formal_education(result):\n",
    "          result['FormalEducation'] = result['FormalEducation'].fillna(\"I prefer not to answer\") #7 tane missing vardı prefer not to answer ile değiştirdim--ali\n",
    "          result['FormalEducation'] = result['FormalEducation'].map({\n",
    "                \"Bachelor's degree\": 1, \"Master's degree\": 2,\n",
    "                 \"Doctoral degree\" : 3, \"Professional degree\": 0,\n",
    "                 \"Some college/university study without earning a bachelor's degree\" :0, \n",
    "                 \"I did not complete any formal education past high school\": 0,\n",
    "                 \"I prefer not to answer\":0\n",
    "\n",
    "            })\n",
    "            #result.fillna(value = 0,inplace = True) --> burayı kaldırdım--ali\n",
    "          return result\n",
    "        \n",
    "        def major_select():\n",
    "            global result\n",
    "            s2 = result.MajorSelect.value_counts().index[:6]\n",
    "            def categ(x):\n",
    "                if x in s2:\n",
    "                    return x\n",
    "                else : \n",
    "                    return \"Missing\" #519 tane missing var. bu şekilde onları others a atıyoruz. %10 olduğu için Missing diye ayrı bir alan yarattım--ali\n",
    "            result['MajorSelect']= result.MajorSelect.apply(lambda x : categ(x))\n",
    "            prefix = 'MajorSelect'\n",
    "            dummies = pd.get_dummies(result['MajorSelect'], prefix=prefix, drop_first=True)# dummy variable trap yüzünden drop_first yaptım--ali\n",
    "            result = pd.concat([result, dummies], axis=1)\n",
    "            result = result.drop(['MajorSelect'], axis=1)\n",
    "            return result\n",
    "        \n",
    "        def DataScienceIdentitySelect():\n",
    "            global result\n",
    "            result = result.fillna({\"DataScienceIdentitySelect\": \"Unanswered\"})\n",
    "            prefix = 'DataScienceIdentitySelect'\n",
    "            dummies = pd.get_dummies(result['DataScienceIdentitySelect'], prefix=prefix, drop_first=True) #bütün oluşturduğumuz alanlarda bu şekilde prefix eklememiz gerekiyor, drop_first yaptım--ali\n",
    "            result = pd.concat([result, dummies], axis=1)\n",
    "            result = result.drop(['DataScienceIdentitySelect'],axis=1)\n",
    "            return result\n",
    "    \n",
    "        def LanguageRecommendationSelect():\n",
    "            global result\n",
    "            result = result.fillna({\"LanguageRecommendationSelect\": \"Unanswered\"})\n",
    "            s = result.LanguageRecommendationSelect.value_counts().index[:4]\n",
    "            def categ(x):#--> missing value ları iki üst satırda unanswered diye değiştirdiğimiz için bu fonksiyona gerek kalmadı--ali\n",
    "                if x in s:\n",
    "                    return x\n",
    "                else :\n",
    "                    return \"Others\"\n",
    "            result['LanguageRecommendationSelect'] = result.LanguageRecommendationSelect.apply(lambda x : categ(x))\n",
    "            prefix = 'LanguageRecommendationSelect'\n",
    "            dummies = pd.get_dummies(result['LanguageRecommendationSelect'],prefix=prefix, drop_first=True) # dummy variable trap yüzünden drop_first yaptım--ali\n",
    "            dummies = dummies.fillna(0)\n",
    "            result = pd.concat([result,dummies], axis=1)\n",
    "            result = result.drop(['LanguageRecommendationSelect'],axis=1)\n",
    "            return result\n",
    "        \n",
    "        def LearningPlatformUsefulness():\n",
    "            global result\n",
    "            col_imputation = ['LearningPlatformUsefulnessBlogs','LearningPlatformUsefulnessKaggle','LearningPlatformUsefulnessCourses',\n",
    "                              'LearningPlatformUsefulnessProjects','LearningPlatformUsefulnessSO','LearningPlatformUsefulnessTextbook','LearningPlatformUsefulnessYouTube']\n",
    "            result[col_imputation] = result[col_imputation].fillna(\"Missing\") #burayı sadece ilgili kolonlara 0 atayacak şekilde düzenledim--ali\n",
    "            for i in result[col_imputation]:\n",
    "                result[i] = result[i].map({\n",
    "                                          \"Missing\": 0,\n",
    "                                          \"Not Useful\": -1,  #missing i ekledikten sonra not useful u -1 e çevirdim--ali\n",
    "                                          \"Somewhat useful\" : 1, \n",
    "                                          \"Very useful\": 2\n",
    "                                           })\n",
    "                \n",
    "            result['mean_of_the_learningplatformusefulness'] = result.filter(regex='LearningPlatformUsefulness').mean(axis=1) #burada 0 lar dahil ortalama alıyoruz, hariç de alabiliriz konuşalım--ali\n",
    "            return result\n",
    "        \n",
    "        \n",
    "        def WorkMLTeamSeatSelect(): #162 tane missing var--ali\n",
    "            global result\n",
    "            result['WorkMLTeamSeatSelect'] = result['WorkMLTeamSeatSelect'].map({#0 : 'Unanswered',-->burada 0: 'unanswered ı missingleeri en basta 0a çekmeyi kaldırdığım için çıkardım--ali                                                         \n",
    "                                                                                'Business Department' : 'Business Department',\n",
    "                                                                                'Central Insights Team' : 'Central Insights Team',\n",
    "                                                                                'IT Department' : 'IT Department' ,\n",
    "                                                                                'Other' : 'Other' ,\n",
    "                                                                                'Standalone Team' : 'Standalone Team'\n",
    "                                                                                })\n",
    "            result['WorkMLTeamSeatSelect'] = result['WorkMLTeamSeatSelect'].fillna('Unanswered') # unanswered ı buraya aldım--ali\n",
    "            prefix = 'WorkMLTeamSeatSelect'\n",
    "            dummies = pd.get_dummies(result['WorkMLTeamSeatSelect'],prefix=prefix, drop_first=True) # dummy variable trap yüzünden drop_first yaptım--ali\n",
    "            result = pd.concat([result,dummies],axis=1)\n",
    "            result.drop('WorkMLTeamSeatSelect', axis=1, inplace=True) #orijinal değişkeni nihai tablodan çıkardım--ali\n",
    "            return result\n",
    "        \n",
    "        \n",
    "        def RemoteWork():#582 tane missing var--ali\n",
    "            global result\n",
    "            result['RemoteWork'] = result['RemoteWork'].map({#0 : 'Unanswered', -->burada 0: 'unanswered ı missingleeri en basta 0a çekmeyi kaldırdığım için çıkardım--ali  \n",
    "                                                             \"\"\"'Don't know'\"\"\" : \"\"\"'Don't know'\"\"\",                                                                  \n",
    "                                                            'Never' : 'Never',\n",
    "                                                            'Rarely' : 'Rarely',\n",
    "                                                            'Sometimes' : 'Sometimes' ,\n",
    "                                                            'Most of the time' : 'Most of the time' ,\n",
    "                                                            'Always' : 'Always'\n",
    "                                                            })\n",
    "            result['RemoteWork'] = result['RemoteWork'].fillna(\"Unanswered\")  # unanswered ı buraya aldım--ali\n",
    "            prefix = 'RemoteWork'\n",
    "            dummies = pd.get_dummies(result['RemoteWork'],prefix=prefix, drop_first=True)# dummy variable trap yüzünden drop_first yaptım--ali\n",
    "            result = pd.concat([result,dummies],axis=1)\n",
    "            result.drop('RemoteWork', axis=1, inplace=True) #orijinal değişkeni nihai tablodan çıkardım--ali\n",
    "            return result\n",
    "        \n",
    "        def GenderSelect():#10 tane missing var male/female dışında cok az data var o yüzden missing ve geri kalanları other olarak atadım--ali \n",
    "            global result\n",
    "            result['GenderSelect'] = result['GenderSelect'].map({\n",
    "                'A different identity' : 'Other',\n",
    "                'Female' : 'Female',\n",
    "                'Male' : 'Male',\n",
    "                'Non-binary, genderqueer, or gender non-conforming' : 'Other'\n",
    "            })\n",
    "            result['GenderSelect'] = result['GenderSelect'].fillna('Other')#missing atamasını burada yaptım--ali\n",
    "            prefix = 'GenderSelect'\n",
    "            dummies = pd.get_dummies(result['GenderSelect'], prefix=prefix, drop_first=True)# dummy variable trap yüzünden drop_first yaptım--ali\n",
    "            result = pd.concat([result,dummies], axis=1)\n",
    "            result.drop('GenderSelect', axis=1, inplace=True) #orijinal değişkeni nihai tablodan çıkardım--ali\n",
    "            return result\n",
    "        \n",
    "        def Country():#16 tane missing vardı o yüzden onları da Other içine koydum--ali\n",
    "            global result\n",
    "            country = ['Pakistan', 'Mexico', 'United States', 'France', 'Other',\n",
    "           'Romania', 'Singapore', 'Argentina', 'Australia',\n",
    "           \"People 's Republic of China\", 'Germany', 'India', 'Russia',\n",
    "           'Taiwan', 'Switzerland', 'United Kingdom', 'Finland', 'Canada',\n",
    "           'Indonesia', 'South Korea', 'Colombia', 'Spain', 'South Africa',\n",
    "           'Italy', 'Brazil', 'Czech Republic', 'Philippines', 'Malaysia',\n",
    "           'Vietnam', 'Egypt', 'Israel', 'Poland', 'Nigeria', 'Chile',\n",
    "           'Belgium', 'Netherlands', 'Ireland', 'Hong Kong', 'Japan',\n",
    "           'Denmark', 'New Zealand', 'Hungary', 'Iran', 'Ukraine', 'Greece',\n",
    "           'Sweden', 'Norway', 'Portugal', 'Kenya', np.nan, 'Turkey',\n",
    "           'Republic of China', 'Belarus']\n",
    "\n",
    "            continent = ['Asia', 'North America', 'North America', 'Europe', 'Other', \n",
    "                 'Europe','Asia', 'South America', 'Oceania',\n",
    "                 'Asia', 'Europe', 'Asia', 'Europe',\n",
    "                 'Asia', 'Europe', 'Europe', 'Europe', 'North America',\n",
    "                 'Asia', 'Asia', 'South America', 'Europe', 'Africa',\n",
    "                 'Europe', 'South America', 'Europe', 'Asia', 'Asia',\n",
    "                 'Asia', 'Africa', 'Asia', 'Europe', 'Africa', 'South America',\n",
    "                 'Europe', 'Europe', 'Europe', 'Asia', 'Asia',\n",
    "                 'Europe', 'Oceania', 'Europe', 'Asia', 'Europe', 'Europe',\n",
    "                 'Europe', 'Europe', 'Europe',  'Africa', np.nan , 'Asia',\n",
    "                 'Asia', 'Europe']\n",
    "            #https://stackoverflow.com/questions/55910004/get-continent-name-from-country-using-pycountry\n",
    "\n",
    "            dic = {}\n",
    "            for x,y in zip(continent, country):\n",
    "                  dic[y] = x\n",
    "            result[\"Country\"] = result[\"Country\"].fillna(\"Other\") #missing atamasını burada yaptım--ali \n",
    "            result['continent'] = result['Country'].map(dic)\n",
    "            prefix = 'continent'\n",
    "            dummies = pd.get_dummies(result['continent'], prefix=prefix, drop_first=True)# dummy variable trap yüzünden drop_first yaptım--ali\n",
    "            result = pd.concat([result,dummies], axis=1)\n",
    "            result.drop(['Country', 'continent'], axis=1, inplace=True) #bu alanları nihai tablodan çıkardım--ali\n",
    "            return result\n",
    "        \n",
    "        \n",
    "        #EmploymentStatus\n",
    "        def hasFullTimeJob():#missing değer yok--ali\n",
    "            global result \n",
    "            result['isFullTimeEmp'] = result['EmploymentStatus'].apply(lambda x: 1 if x == 'Employed full-time'  else 0)\n",
    "            result.drop('EmploymentStatus', axis=1, inplace=True)\n",
    "            return result\n",
    "            #Employment Status drop edilmedi. edilebilir--ali\n",
    "         \n",
    "        \n",
    "        #CodeWriter\n",
    "        def CodeWriter():\n",
    "            global result\n",
    "            result.drop('CodeWriter', axis=1, inplace=True)\n",
    "            return result\n",
    "        \n",
    "        #CurrentJobTitleSelect\n",
    "        def JobTitleGrouping():\n",
    "          global result\n",
    "          #group by similar job satisfaction levels\n",
    "          group1 = ['Software Developer/Software Engineer', 'Computer Scientist', 'Data Miner', 'Engineer', 'Business Analyst', 'DBA/Database Engineer', 'Programmer', \"Data Analyst\" ]\n",
    "          group2 = ['Scientist/Researcher', 'Data Scientist', 'Statistician', 'Operations Research Practitioner']\n",
    "          group3 = ['Other', 'Predictive Modeler', 'Researcher', 'Machine Learning Engineer']\n",
    "\n",
    "          groups_dict = {}\n",
    "\n",
    "          for i in group1:\n",
    "            groups_dict[i] = 'group1'\n",
    "\n",
    "          for i in group2:\n",
    "            groups_dict[i] = 'group2'\n",
    "\n",
    "          for i in group3:\n",
    "            groups_dict[i] = 'group3'\n",
    "\n",
    "          result['CurrentJobTitleSelect'] = result['CurrentJobTitleSelect'].map(groups_dict).fillna('Missing')\n",
    "\n",
    "          from sklearn.preprocessing import OneHotEncoder\n",
    "          ohe = OneHotEncoder(drop='first')# dummy variable trap yüzünden drop_first yaptım--ali\n",
    "\n",
    "          encoded = pd.DataFrame(ohe.fit_transform(result[['CurrentJobTitleSelect']]).toarray())\n",
    "          encoded.columns = ['CurrentJobTitleSelect_group1','CurrentJobTitleSelect_group2', 'CurrentJobTitleSelect_group3']\n",
    "          result = pd.concat([result, encoded], axis=1)\n",
    "          result = result.drop('CurrentJobTitleSelect', axis=1) \n",
    "          return result\n",
    "        \n",
    "      \n",
    "    \n",
    "        #WorkProductionFrequency\n",
    "        def ProductionFreq(): #626 missing değer var\n",
    "          global result\n",
    "          #ProductionFreqDict = {\"Never\": 0 ,  \"Don't know\":0, 'Rarely':1, 'Sometimes':2, 'Most of the time':3,'Always':4 } #dont know 0 mu kalmalı\n",
    "          ProductionFreqDict = {\"Never\": -1 ,  \"Don't know\":0, 'Rarely':1, 'Sometimes':2, 'Most of the time':3,'Always':4 } #never ı -1 yaptım--ali\n",
    "          result[\"WorkProductionFrequency\"] = result[\"WorkProductionFrequency\"].map(ProductionFreqDict)\n",
    "          result[\"WorkProductionFrequency\"] = result[\"WorkProductionFrequency\"].fillna(0) #bu şekilde missing ler de dont know ile aynı değeri almış oluyor--ali\n",
    "          return result\n",
    "    \n",
    "    \n",
    "        #EmployerSize\n",
    "        def EmployerSizeGrouping(): \n",
    "          #581 tane missing var. bunları ayrıca ekleyelim bence. ek olarak, şirket büyüklüğü ile job satisfaction arasında ordinal bir bağ kuruyoruz. bence doğru değil. \n",
    "          #o yüzden alternatif olarak onehotencoding yapabiliriz--ali\n",
    "          global result\n",
    "          sizeDict = { 'I prefer not to answer':0,\n",
    "               \"I don't know\":0,\n",
    "              \"100 to 499 employees\":1, \n",
    "              \"Fewer than 10 employees\" : 1 , \n",
    "              \"10 to 19 employees\":1,\n",
    "              '20 to 99 employees':1,\n",
    "              \"500 to 999 employees\":2,\n",
    "              '1,000 to 4,999 employees':3,\n",
    "              '5,000 to 9,999 employees':4,\n",
    "              '10,000 or more employees':5}\n",
    "\n",
    "          #result['EmployerSize'] = result['EmployerSize'].map(sizeDict) #--> bu kodu kaldırdım fakat alan adlarını değiştirmek istersek kullanabiliriz--ali\n",
    "          result['EmployerSize'] = result['EmployerSize'].fillna(\"Missing\") #missingleri ayrı bir alan olarak atadım--ali          \n",
    "          prefix = 'EmployerSize'\n",
    "          dummies = pd.get_dummies(result['EmployerSize'], prefix=prefix, drop_first=True)# dummy variable trap yüzünden drop_first yaptım--ali\n",
    "          result = pd.concat([result,dummies], axis=1)\n",
    "          result.drop('EmployerSize', axis=1, inplace=True) #orijinal değişkeni çıkardım--ali\n",
    "       \n",
    "          return result\n",
    "    \n",
    "        def dumyfier(): #burada bilerek drop first uygulamadım --ali\n",
    "            global result\n",
    "            columns = [\"PastJobTitlesSelect\",\"CurrentEmployerType\",\"MLSkillsSelect\",\"WorkAlgorithmsSelect\",\"MLTechniquesSelect\"]\n",
    "            def dummy_df(result, todummy_list):\n",
    "                for x in todummy_list:\n",
    "                    prefix = x\n",
    "                    dummies = result[x].str.get_dummies(sep=',').add_prefix(prefix+'_') #duplike alan adlarını önlemek için buraya prefix ekledim--ali\n",
    "                    result = result.drop(x, 1)\n",
    "                    result = pd.concat([result, dummies], axis=1)\n",
    "                return result\n",
    "            result = dummy_df(result, columns)\n",
    "\n",
    "            pastjobtitle_group1 = [\"PastJobTitlesSelect_Researcher\", \"PastJobTitlesSelect_Other\", \"PastJobTitlesSelect_Data Scientist\", \"PastJobTitlesSelect_Computer Scientist\"]\n",
    "\n",
    "            pastjobtitle_group2 = [\"PastJobTitlesSelect_Software Developer/Software Engineer\", \"PastJobTitlesSelect_Data Analyst\", \"PastJobTitlesSelect_Engineer\", \"PastJobTitlesSelect_Programmer\", \n",
    "                                  \"PastJobTitlesSelect_Business Analyst\", \"PastJobTitlesSelect_I haven't started working yet\"]\n",
    "\n",
    "            pastjobtitle_group3 = [\"PastJobTitlesSelect_Data Miner\", \"PastJobTitlesSelect_DBA/Database Engineer\", \"PastJobTitlesSelect_Machine Learning Engineer\", \n",
    "                                  \"PastJobTitlesSelect_Operations Research Practitioner\", \"PastJobTitlesSelect_Predictive Modeler\", \"PastJobTitlesSelect_Statistician\",\n",
    "                                  \"PastJobTitlesSelect_Engineer\"]\n",
    "\n",
    "            currentemployertype_group1 = [\"CurrentEmployerType_Employed by a company that performs advanced analytics\", \"CurrentEmployerType_Employed by college or university\",\n",
    "                                          \"CurrentEmployerType_Employed by company that makes advanced analytic software\", \"CurrentEmployerType_Self-employed\"]\n",
    "\n",
    "            currentemployertype_group2 = [\"CurrentEmployerType_Employed by a company that doesn't perform advanced analytics\", \"CurrentEmployerType_Employed by government\",\n",
    "                                          \"CurrentEmployerType_Employed by non-profit or NGO\", \"CurrentEmployerType_Employed by professional services/consulting firm\",\n",
    "                                          ]\n",
    "\n",
    "            mlskilss_group1 = [\"MLSkillsSelect_Adversarial Learning\", \"MLSkillsSelect_Natural Language Processing\", \"MLSkillsSelect_Other (please specify; separate by semi-colon)\",\n",
    "                              \"MLSkillsSelect_Outlier detection (e.g. Fraud detection)\", \"MLSkillsSelect_Recommendation Engines\", \"MLSkillsSelect_Reinforcement learning\",\n",
    "                              \"MLSkillsSelect_Survival Analysis\", \"MLSkillsSelect_Time Series\", \"MLSkillsSelect_Unsupervised Learning\"]\n",
    "\n",
    "            mlskilss_group2 = [\"MLSkillsSelect_Computer Vision\", \"MLSkillsSelect_Machine Translation\", \"MLSkillsSelect_Speech Recognition\", \"MLSkillsSelect_Supervised Machine Learning (Tabular Data)\",\n",
    "                              ]\n",
    "\n",
    "\n",
    "            workalgorithms_group1 = [\"WorkAlgorithmsSelect_CNNs\", \"WorkAlgorithmsSelect_Decision Trees\", \"WorkAlgorithmsSelect_Ensemble Methods\", \"WorkAlgorithmsSelect_Gradient Boosted Machines\",\n",
    "                                    \"WorkAlgorithmsSelect_Neural Networks\", \"WorkAlgorithmsSelect_RNNs\", \"WorkAlgorithmsSelect_Random Forests\", \"WorkAlgorithmsSelect_SVMs\"]\n",
    "\n",
    "            workalgorithms_group2 = [\"WorkAlgorithmsSelect_Other\", \"WorkAlgorithmsSelect_Regression/Logistic Regression\"]\n",
    "\n",
    "            workalgorithms_group3 = [\"WorkAlgorithmsSelect_Bayesian Techniques\", \"WorkAlgorithmsSelect_Evolutionary Approaches\", \"WorkAlgorithmsSelect_GANs\", \"WorkAlgorithmsSelect_HMMs\",\n",
    "                                    \"WorkAlgorithmsSelect_Markov Logic Networks\"]\n",
    "\n",
    "\n",
    "\n",
    "            mltechniques_group1 = [\"MLTechniquesSelect_Bayesian Techniques\", \"MLTechniquesSelect_Decision Trees - Gradient Boosted Machines\",\"MLTechniquesSelect_Ensemble Methods\", \n",
    "                                  \"MLTechniquesSelect_Evolutionary Approaches\", \"MLTechniquesSelect_Gradient Boosting\", \"MLTechniquesSelect_Hidden Markov Models HMMs\", \"MLTechniquesSelect_Markov Logic Networks\",\n",
    "                                  \"MLTechniquesSelect_Neural Networks - CNNs\", \"MLTechniquesSelect_Neural Networks - GANs\", \"MLTechniquesSelect_Neural Networks - RNNs\", \"MLTechniquesSelect_Other (please specify; separate by semi-colon)\",\n",
    "                                  \"MLTechniquesSelect_Support Vector Machines (SVMs)\"]\n",
    "\n",
    "            mltechniques_group2 = [\"MLTechniquesSelect_Decision Trees - Random Forests\", \"MLTechniquesSelect_Logistic Regression\"]\n",
    "\n",
    "            result[\"pastjobtitle_group1\"] = result[pastjobtitle_group1].sum(axis=1)\n",
    "            result[\"pastjobtitle_group2\"] = result[pastjobtitle_group2].sum(axis=1)\n",
    "            result[\"pastjobtitle_group3\"] = result[pastjobtitle_group3].sum(axis=1)\n",
    "\n",
    "            result[\"currentemployertype_group1\"] = result[currentemployertype_group1].sum(axis=1)\n",
    "            result[\"currentemployertype_group2\"] = result[currentemployertype_group2].sum(axis=1)\n",
    "\n",
    "            result[\"mlskills_group1\"] = result[mlskilss_group1].sum(axis=1)\n",
    "            result[\"mlskills_group2\"] = result[mlskilss_group2].sum(axis=1)\n",
    "\n",
    "            result[\"workalgorithms_group1\"] = result[workalgorithms_group1].sum(axis=1)\n",
    "            result[\"workalgorithms_group2\"] = result[workalgorithms_group2].sum(axis=1)\n",
    "            result[\"workalgorithms_group3\"] = result[workalgorithms_group3].sum(axis=1)\n",
    "\n",
    "            result[\"mltechniques_group1\"] = result[mltechniques_group1].sum(axis=1)\n",
    "            result[\"mltechniques_group2\"] = result[mltechniques_group2].sum(axis=1)\n",
    "\n",
    "            drop_list = [pastjobtitle_group1, pastjobtitle_group2, pastjobtitle_group3, currentemployertype_group1, currentemployertype_group2, mlskilss_group1, \n",
    "                            mlskilss_group2, workalgorithms_group1, workalgorithms_group2, workalgorithms_group3, mltechniques_group1, mltechniques_group2]\n",
    "\n",
    "            drop_list = [item for sublist in drop_list for item in sublist]                \n",
    "            result = result.drop(drop_list, axis=1)\n",
    "\n",
    "            return result\n",
    "        \n",
    "        def EmplIndustry(): #bu değişken için fonksiyon yazmamışız. ekledim. --> 12 tane missing var\n",
    "            global result\n",
    "            result = result.fillna({\"EmployerIndustry\": \"Unanswered\"})\n",
    "            prefix = 'EmployerIndustry'\n",
    "            dummies = pd.get_dummies(result['EmployerIndustry'], prefix=prefix, drop_first=True)#bütün oluşturduğumuz alanlarda bu şekilde prefix eklememiz gerekiyor--ali\n",
    "            result = pd.concat([result, dummies], axis=1)\n",
    "            result = result.drop(['EmployerIndustry'],axis=1)\n",
    "            return result\n",
    "\n",
    "        def RemoveID():#bu alan nihai tablodan çıkarılmamıştı. çıkardım--ali\n",
    "          global result\n",
    "          result.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "        def EncodeTitleFit(): #encode edilmemişti. ettim--ali\n",
    "          global result\n",
    "          result['TitleFit'].fillna('Missing', inplace=True)\n",
    "          result['TitleFit'] = result['TitleFit'].map({\"Fine\": 1, \"Perfectly\": 2, \"Poorly\": -1, 'Missing': 0})\n",
    "          return result\n",
    "\n",
    "        def TenureEncode():\n",
    "          global result\n",
    "          result['Tenure'] = result['Tenure'].fillna(\"I don't write code to analyze data\")#çok az missing var onlara da böyle bir kabul yaparak ilerledim--ali\n",
    "          result['Tenure'] = result['Tenure'].map({'3 to 5 years':3,\n",
    "                            '1 to 2 years':2,\n",
    "                            'More than 10 years':5,\n",
    "                            '6 to 10 years':4,\n",
    "                            'Less than a year':1,\n",
    "                            \"I don't write code to analyze data\":0\n",
    "                            })\n",
    "          return result\n",
    "\n",
    "        def Compensation():\n",
    "          global result\n",
    "          result['CompensationMissingIndicator'] = result[\"CompensationScore\"].isnull().replace({True: 1, False: 0})\n",
    "          result['CompensationScore'] = result['CompensationScore'].fillna(1000)\n",
    "          return result\n",
    "        \n",
    "\n",
    "        def JobSatisfaction():\n",
    "            global result\n",
    "            result['JobSatisfaction'] = result['JobSatisfaction'].map({\n",
    "            'Low' : 1,\n",
    "            'Medium' : 2,\n",
    "            'High' : 3\n",
    "                })\n",
    "            return result\n",
    "        \n",
    "        \n",
    "        def colum_dropper():\n",
    "            global result\n",
    "            result = result[result.columns.drop(list(result.filter(regex='LearningPlatformUsefulness')))]\n",
    "            result = result[result.columns.drop(list(result.filter(regex='WorkToolsFrequency')))]\n",
    "            result = result[result.columns.drop(list(result.filter(regex='WorkMethodsFrequency')))]\n",
    "            result = result[result.columns.drop(list(result.filter(regex='WorkChallengeFrequency')))]\n",
    "            return  result\n",
    "        \n",
    "        def Age_null_imputer():#PCA ONCESINDE NULLAR ICIN EKLENDI PIPELINE DISINDA SIMPLE IMPUTER CALISMIYOR EDILMIYO\n",
    "            global result\n",
    "            result[\"Age\"] = result[\"Age\"].fillna(result[\"Age\"].mean())\n",
    "            return result\n",
    "\n",
    "\n",
    "        \n",
    "        column_imputer()\n",
    "        worktoolsfreqmean()\n",
    "        workchallenge()\n",
    "        visualization(result)\n",
    "        externalvsinternaltools(result)\n",
    "        formal_education(result)\n",
    "        major_select()\n",
    "        DataScienceIdentitySelect()\n",
    "        LanguageRecommendationSelect()\n",
    "        LearningPlatformUsefulness()\n",
    "        WorkMLTeamSeatSelect()\n",
    "        RemoteWork()\n",
    "        GenderSelect()\n",
    "        Country()\n",
    "        hasFullTimeJob()\n",
    "        CodeWriter()\n",
    "        JobTitleGrouping()\n",
    "        ProductionFreq()\n",
    "        dumyfier()\n",
    "        EmployerSizeGrouping()\n",
    "        EmplIndustry()\n",
    "        RemoveID()        \n",
    "        EncodeTitleFit()\n",
    "        TenureEncode()\n",
    "        Compensation()\n",
    "        colum_dropper()\n",
    "        Age_null_imputer()\n",
    "\n",
    "\n",
    "        JobSatisfaction()\n",
    "        print(\"Feature Selection Process Succesfully Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Selection Process is started\n",
      "Feature Selection Process Succesfully Completed\n"
     ]
    }
   ],
   "source": [
    "pipeline_feature_generation = Pipeline([('feature generator', imputer_and_generator._main_())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.drop([\"MLToolNextYearSelect\",\"MLMethodNextYearSelect\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = result['JobSatisfaction']\n",
    "X = result.drop(\"JobSatisfaction\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: Split the training test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "tree = DecisionTreeClassifier(random_state=seed)\n",
    "models = [\n",
    "    ['DecisionTree', DecisionTreeClassifier(random_state=seed)], \n",
    "    ['RandomForest', RandomForestClassifier(n_estimators=100, random_state=seed)], #default changes to 100 in sklearn 0.22\n",
    "    ['ExtraTrees', ExtraTreesClassifier(n_estimators=100, random_state=seed)],\n",
    "    ['BaggingClassifier', BaggingClassifier(base_estimator=tree, n_estimators=100, random_state=seed)],\n",
    "    ['AdaBoost', AdaBoostClassifier(n_estimators=100, random_state=seed)], \n",
    "    ['GradientBoosting', GradientBoostingClassifier(random_state=seed)],\n",
    "    ['XGBoost', XGBClassifier(seed=seed)],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     DecisionTree  0.425\n",
      "     RandomForest  0.500\n",
      "       ExtraTrees  0.476\n",
      "BaggingClassifier  0.484\n",
      "         AdaBoost  0.503\n",
      " GradientBoosting  0.508\n",
      "          XGBoost  0.471\n"
     ]
    }
   ],
   "source": [
    "# Step3: Use the following classifiers with their defaults\n",
    "for abbrv, clf in models:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('%17s  %5.3f' % (abbrv,f1_score(y_test, y_pred, pos_label=1,average=\"micro\")))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     DecisionTree  0.425\n",
      "     RandomForest  0.500\n",
      "       ExtraTrees  0.476\n",
      "BaggingClassifier  0.484\n",
      "         AdaBoost  0.503\n",
      " GradientBoosting  0.508\n",
      "          XGBoost  0.471\n"
     ]
    }
   ],
   "source": [
    "# Step3: Use the following classifiers with their defaults\n",
    "for abbrv, clf in models:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('%17s  %5.3f' % (abbrv,accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed =10\n",
    "def tune(model, param_grid, gs, seed):\n",
    "    if gs == 'rand':\n",
    "        grid = RandomizedSearchCV(model, param_grid, cv = 5, n_iter=20, n_jobs = -1, random_state=seed)\n",
    "    else:\n",
    "        grid = GridSearchCV(model, param_grid, cv = 5, n_jobs = -1)\n",
    "    start = time.time()\n",
    "    # Fit the grid search to the data\n",
    "    grid.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print(grid.best_params_)\n",
    "    print('Execution time:', end-start, 'sec\\n')\n",
    "    best_grid = grid.best_estimator_\n",
    "    train_accuracy = accuracy_score(y_train, best_grid.predict(X_train))\n",
    "    test_accuracy  = accuracy_score(y_test, best_grid.predict(X_test))\n",
    "    print('Train accuracy:', train_accuracy)\n",
    "    print('Test accuracy :', test_accuracy)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'max_depth': 10, 'criterion': 'gini', 'bootstrap': True}\n",
      "Execution time: 12.844653367996216 sec\n",
      "\n",
      "Train accuracy: 0.6726965750120598\n",
      "Test accuracy : 0.5054229934924078\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['entropy', 'gini'],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'max_depth': [10,20],\n",
    "    'min_samples_leaf': [10,20],\n",
    "    'min_samples_split': [5,100],\n",
    "    'bootstrap': [True, False]}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestClassifier(n_estimators = 100, random_state=seed)\n",
    "grid_rf = tune(rf, param_grid, 'rand', seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.7, 'n_estimators': 60, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_depth': 2, 'learning_rate': 0.3}\n",
      "Execution time: 163.5677580833435 sec\n"
     ]
    }
   ],
   "source": [
    "random_grid = {\n",
    "    'n_estimators'     : [40, 50, 60, 70, 80],\n",
    "    'learning_rate'    : [0.25, 0.3, 0.35],\n",
    "    'subsample'        : [0.6, 0.7, 0.8],\n",
    "    'min_samples_split': [3, 5, 7],\n",
    "    'min_samples_leaf' : [5, 10, 15],\n",
    "    'max_depth'        : [2, 3, 5, 8]}\n",
    "\n",
    "grid_gbm = RandomizedSearchCV(estimator=GradientBoostingClassifier(random_state=seed), n_iter=20,\n",
    "                              param_distributions = random_grid, cv = 10, random_state=seed, n_jobs = -1)\n",
    "start = time.time()\n",
    "grid_gbm.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(grid_gbm.best_params_)\n",
    "print('Execution time:', end-start, 'sec')\n",
    "best_grid = grid_gbm.best_estimator_\n",
    "train_accuracy = accuracy_score(y_train, best_grid.predict(X_train))\n",
    "test_accuracy  = accuracy_score(y_test, best_grid.predict(X_test))\n",
    "\n",
    "print('Train accuracy:', train_accuracy)\n",
    "print('Test accuracy :', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.5988904968644476\n",
      "Test accuracy : 0.49963846710050613\n"
     ]
    }
   ],
   "source": [
    "best_grid = grid_gbm.best_estimator_\n",
    "train_accuracy = accuracy_score(y_train, best_grid.predict(X_train))\n",
    "test_accuracy  = accuracy_score(y_test, best_grid.predict(X_test))\n",
    "\n",
    "print('Train accuracy:', train_accuracy)\n",
    "print('Test accuracy :', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 90}\n",
      "Best CV score  : 0.5236411992965833\n"
     ]
    }
   ],
   "source": [
    "params = {'max_depth': [2, 3, 4, 5, None], \n",
    "          \"learning_rate\" : [0.05,0.30],\n",
    "         \"n_estimators\": [10,90,150]}\n",
    "\n",
    "model = GradientBoostingClassifier(random_state=seed)\n",
    "\n",
    "gridSearch2 = GridSearchCV(estimator = model, param_grid = params, scoring='f1_micro', n_jobs=-1)\n",
    "gridSearch2.fit(X_train,y_train)\n",
    "print('Best parameters:', gridSearch2.best_params_)\n",
    "print('Best CV score  :', gridSearch2.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
